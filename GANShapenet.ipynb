{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-19T07:02:10.196314",
     "start_time": "2017-05-19T00:02:03.396911-07:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%aimport util\n",
    "from util import *\n",
    "%autoreload 1\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-19T07:02:57.360943",
     "start_time": "2017-05-19T00:02:10.196314-07:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delighted_dirs, lighted_dirs = scan_lighted_delighted('data/')\n",
    "delighted_data, lighted_data = load_dataset(delighted_dirs, lighted_dirs)\n",
    "test_meshes = ['Mesh_000003', 'Mesh_000006']\n",
    "train_lighted_data = [(label, arr) for label, arr in lighted_data if label not in test_meshes]\n",
    "test_lighted_data = [(label, arr) for label, arr in lighted_data if label in test_meshes]\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(train_lighted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-19T07:03:03.245012",
     "start_time": "2017-05-19T00:03:03.227999-07:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GEN_ENCODER = 'gen_encoder'\n",
    "GEN_MID = 'gen_mid'\n",
    "GEN_DECODER = 'gen_decoder'\n",
    "DISC_CONV = 'disc_conv'\n",
    "DISC_FC = 'disc_fc'\n",
    "\n",
    "def get_dimension_name(stage, layer_num):\n",
    "    return 'dims_' + stage + '_' + str(layer_num)\n",
    "\n",
    "def get_weight_name(stage, layer_num):\n",
    "    return 'w_' + stage + '_' + str(layer_num)\n",
    "    \n",
    "def get_bias_name(stage, layer_num):\n",
    "    return 'b_' + stage + '_' + str(layer_num)\n",
    "\n",
    "FULL_HEIGHT = 1024\n",
    "TRAIN_HEIGHT = 32\n",
    "TEST_HEIGHT = FULL_HEIGHT\n",
    "\n",
    "train_input_dimensions = (TRAIN_HEIGHT, TRAIN_HEIGHT, 3)\n",
    "test_input_dimensions = (TEST_HEIGHT, TEST_HEIGHT, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-19T07:03:03.818514",
     "start_time": "2017-05-19T00:03:03.747958-07:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc(x, output_depth, name, activation=tf.nn.relu):\n",
    "    input_depth = int(x.get_shape()[-1])\n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.get_variable('W', shape=[input_depth, output_depth], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.get_variable('b', [output_depth], initializer=tf.zeros_initializer())\n",
    "        return activation(tf.matmul(x, W) + b)\n",
    "    \n",
    "def conv2d(x, output_depth, name, filter_size=3, stride=1, padding='SAME'):\n",
    "    input_depth = int(x.get_shape()[-1])\n",
    "\n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.get_variable('W',\n",
    "                            shape=[filter_size, filter_size, input_depth, output_depth],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.get_variable('b', shape=[output_depth], initializer=tf.zeros_initializer())\n",
    "        conv_output = tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding=padding) + b\n",
    "        return tf.nn.relu(conv_output)\n",
    "\n",
    "def deconv2d(x, output_depth, name, filter_size=3, stride=1, padding='SAME'):\n",
    "    _, old_height, old_width, input_depth = x.get_shape().as_list()\n",
    "    x = conv2d(x, output_depth, name, filter_size=filter_size, stride=stride, padding=padding)\n",
    "    with tf.variable_scope(name):\n",
    "        # bilinear interpolation upsampling\n",
    "        scale = 2\n",
    "        new_height = old_height * scale\n",
    "        new_width = old_width * scale\n",
    "        return tf.image.resize_images(x, [new_height, new_width], method=tf.image.ResizeMethod.BILINEAR)\n",
    "\n",
    "def gradient_difference_loss(expected, predicted, alpha):\n",
    "    pos = tf.constant(np.identity(3), dtype=tf.float32)\n",
    "    filter_x = tf.expand_dims(tf.stack([-pos, pos]), 0)  # [-1, 1]\n",
    "    filter_y = tf.stack([tf.expand_dims(pos, 0), tf.expand_dims(-pos, 0)])  # [[1], [-1]]\n",
    "\n",
    "    predicted_dx = tf.abs(tf.nn.conv2d(predicted, filter_x, [1, 1, 1, 1], padding='SAME'))\n",
    "    predicted_dy = tf.abs(tf.nn.conv2d(predicted, filter_y, [1, 1, 1, 1], padding='SAME'))\n",
    "    expected_dx = tf.abs(tf.nn.conv2d(expected, filter_x, [1, 1, 1, 1], padding='SAME'))\n",
    "    expected_dy = tf.abs(tf.nn.conv2d(expected, filter_y, [1, 1, 1, 1], padding='SAME'))\n",
    "\n",
    "    grad_diff_x = tf.abs(expected_dx - predicted_dx)\n",
    "    grad_diff_y = tf.abs(expected_dy - predicted_dy)\n",
    "\n",
    "    return tf.reduce_mean(grad_diff_x ** alpha + grad_diff_y ** alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-19T07:03:04.948189",
     "start_time": "2017-05-19T00:03:04.540198-07:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GeneratorNetwork(object):\n",
    "    def __init__(self, session, train_input_dimensions, test_input_dimensions,\n",
    "                 hyperparameters=None,\n",
    "                 input_residual=False,\n",
    "                 alpha_mask_loss=False):\n",
    "        self.sess = session\n",
    "        self.train_input_dimensions = train_input_dimensions\n",
    "        self.test_input_dimensions = test_input_dimensions\n",
    "        if not hyperparameters:\n",
    "            hyperparameters = {\n",
    "                'learning_rate' : 1e-3,\n",
    "                'gdl_alpha' : 1,\n",
    "                'lambda_l2' : 1,\n",
    "                'lambda_gdl' : 1,\n",
    "                'lambda_adv' : 1\n",
    "            }\n",
    "        self.hyperparams = hyperparameters\n",
    "        self.train_variables = []\n",
    "        self.has_defined_layers = False\n",
    "        self.input_residual = input_residual\n",
    "        self.alpha_mask_loss = alpha_mask_loss\n",
    "    \n",
    "    def init_network(self, discriminator):\n",
    "        train_width, train_height, train_depth = self.train_input_dimensions\n",
    "        self.train_input = tf.placeholder(tf.float32, shape=[None, train_width, train_height, train_depth])\n",
    "        self.alpha_mask = tf.placeholder(tf.float32, shape=[None, train_width, train_height, 1])\n",
    "        self.test_input = tf.placeholder(tf.float32, shape=[None,] + list(self.test_input_dimensions))\n",
    "        self.expected_output = tf.placeholder(tf.float32, shape=[None,] + list(self.train_input_dimensions))\n",
    "        \n",
    "        train_output = self.get_output_tensor(self.train_input)\n",
    "        self.test_output = self.get_output_tensor(self.test_input)\n",
    "        \n",
    "        sq_diff = tf.squared_difference(train_output, self.expected_output)\n",
    "        if self.alpha_mask_loss:\n",
    "            sq_diff *= self.alpha_mask\n",
    "        l2_loss = tf.reduce_mean(sq_diff)\n",
    "        gdl_loss = gradient_difference_loss(train_output, self.expected_output, self.hyperparams['gdl_alpha'])\n",
    "        adv_loss = -tf.reduce_mean(tf.log(discriminator.get_output_tensor(train_output)))\n",
    "        self.loss = self.hyperparams['lambda_l2'] * l2_loss \\\n",
    "                  + self.hyperparams['lambda_gdl'] * gdl_loss \\\n",
    "                  + self.hyperparams['lambda_adv'] * adv_loss\n",
    "        self.opt = tf.train.AdamOptimizer(learning_rate=self.hyperparams['learning_rate']).minimize(self.loss, var_list=self.train_variables)\n",
    "        with tf.name_scope('generator'):\n",
    "            l2_loss_summ = tf.summary.scalar('l2_loss', l2_loss)\n",
    "            gdl_loss_summ = tf.summary.scalar('gradient_difference_loss', gdl_loss)\n",
    "            adversarial_loss_summ = tf.summary.scalar('adversarial_loss', adv_loss)\n",
    "            loss_summ = tf.summary.scalar('loss', self.loss)\n",
    "            self.summaries = tf.summary.merge([l2_loss_summ, gdl_loss_summ, adversarial_loss_summ, loss_summ])\n",
    "        \n",
    "    def get_output_tensor(self, input):\n",
    "        with tf.variable_scope('generator', reuse=self.has_defined_layers):\n",
    "            encoder0 = conv2d(input, 4, 'encoder_0')\n",
    "            encoder1 = conv2d(encoder0, 8, 'encoder_1', stride=2)\n",
    "            encoder2 = conv2d(encoder1, 16, 'encoder_2', stride=2)\n",
    "            mid0 = conv2d(encoder2, 16, 'mid_0')\n",
    "            decoder0 = deconv2d(tf.concat([mid0, encoder2], axis=3), 8, 'decoder_0')\n",
    "            decoder1 = deconv2d(tf.concat([decoder0, encoder1], axis=3), 4, 'decoder_1')\n",
    "            if self.input_residual:\n",
    "                output = input + conv2d(tf.concat([decoder1, encoder0], axis=3), 3, 'decoder_2')\n",
    "            else:\n",
    "                output = conv2d(decoder1, 3, 'decoder_2')\n",
    "        if not self.has_defined_layers:\n",
    "            self.train_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator')\n",
    "            self.has_defined_layers = True\n",
    "        return output\n",
    "    \n",
    "    def fit_batch(self, inputs, expected_outputs, alpha):\n",
    "        _, loss, summaries = self.sess.run((self.opt, self.loss, self.summaries), feed_dict={self.train_input : inputs, self.expected_output : expected_outputs, self.alpha_mask : alpha })\n",
    "        return loss, summaries\n",
    "                \n",
    "    def predict(self, inputs):\n",
    "        return self.sess.run(self.test_output, feed_dict={self.test_input : inputs})\n",
    "    \n",
    "class DiscriminatorNetwork(object):\n",
    "    def __init__(self, session, train_input_dimensions, learning_rate=1e-3):\n",
    "        self.sess = session\n",
    "        self.train_input_dimensions = train_input_dimensions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_variables = []\n",
    "        self.has_defined_layers = False\n",
    "    \n",
    "    def init_network(self, generator):\n",
    "        self.lighted_input = tf.placeholder(tf.float32, shape=[None,] + list(self.train_input_dimensions))\n",
    "        self.delighted_input = tf.placeholder(tf.float32, shape=[None,] + list(self.train_input_dimensions))\n",
    "        real_input = self.delighted_input\n",
    "        fake_input = generator.get_output_tensor(self.lighted_input)\n",
    "        \n",
    "        predicted_real = self.get_output_tensor(real_input)\n",
    "        predicted_fake = self.get_output_tensor(fake_input)\n",
    "        \n",
    "        real_loss = -tf.reduce_mean(tf.log(predicted_real))\n",
    "        fake_loss = -tf.reduce_mean(tf.log(1 - predicted_fake))\n",
    "        \n",
    "        self.loss = real_loss + fake_loss\n",
    "        self.opt = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss, var_list=self.train_variables)\n",
    "        with tf.name_scope('discriminator'):\n",
    "            real_loss_summ = tf.summary.scalar('real_loss', real_loss)\n",
    "            fake_loss_summ = tf.summary.scalar('fake_loss', fake_loss)\n",
    "            loss_summ = tf.summary.scalar('loss', self.loss)\n",
    "            self.summaries = tf.summary.merge([real_loss_summ, fake_loss_summ, loss_summ])\n",
    "            \n",
    "    def get_output_tensor(self, input):\n",
    "        '''\n",
    "        Given an input tensor / placeholder, perform convs then FCs to get the probability that input is real\n",
    "        '''\n",
    "        with tf.variable_scope('discriminator', reuse=self.has_defined_layers):\n",
    "            conv0 = conv2d(input, 4, 'conv_0', stride=2)\n",
    "            conv1 = conv2d(conv0, 8, 'conv_1', stride=2)\n",
    "            conv2 = conv2d(conv1, 16, 'conv_2', stride=2)\n",
    "            fc_input = tf.contrib.layers.flatten(conv2)\n",
    "            fc_0 = fc(fc_input, 256, 'fc_0')\n",
    "            fc_1 = fc(fc_0, 128, 'fc_1')\n",
    "            output = fc(fc_1, 1, 'fc_2', activation=tf.nn.sigmoid)\n",
    "        if not self.has_defined_layers:\n",
    "            self.train_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='discriminator')\n",
    "            self.has_defined_layers = True\n",
    "        return output\n",
    "        \n",
    "    def fit_batch(self, lighted_inputs, delighted_inputs):\n",
    "        _, loss, summaries = self.sess.run((self.opt, self.loss, self.summaries), feed_dict={ self.lighted_input : lighted_inputs, self.delighted_input : delighted_inputs })\n",
    "        return loss, summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-19T07:03:05.814933",
     "start_time": "2017-05-19T00:03:05.760897-07:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_batch(delighted_data, lighted_data, batch_size, i):\n",
    "    inputs = []\n",
    "    expected_outputs = []\n",
    "    alphas = []\n",
    "    for i, (mesh, input) in enumerate(lighted_data[i * batch_size : (i + 1) * batch_size]):\n",
    "        while True:\n",
    "            start_x, start_y = np.random.randint(0, FULL_HEIGHT - TRAIN_HEIGHT + 1, size=(2,))\n",
    "            sample = input[start_x : start_x + TRAIN_HEIGHT, start_y : start_y + TRAIN_HEIGHT]\n",
    "            if np.sum(sample > 0) > 0.1 * TRAIN_HEIGHT ** 2:\n",
    "                rotate_rand = np.random.randint(4)\n",
    "                flip_rand = np.random.randint(0, 1, size=1)[0]\n",
    "                sample = np.rot90(sample, rotate_rand)\n",
    "                delighted = delighted_data[mesh][start_x : start_x + TRAIN_HEIGHT, start_y : start_y + TRAIN_HEIGHT]\n",
    "                if flip_rand:\n",
    "                    sample = np.flip(sample, axis=0)\n",
    "                    delighted = np.flip(delighted, axis=0)\n",
    "                inputs.append(sample)\n",
    "                expected_outputs.append(delighted)\n",
    "                alphas.append(np.any(delighted, axis=2, keepdims=True).astype(int))\n",
    "                break\n",
    "    return np.asarray(expected_outputs), np.asarray(inputs), np.asarray(alphas)\n",
    "\n",
    "def get_test_batch(delighted_data, lighted_data, batch_size, i):\n",
    "    meshes, inputs = zip(*lighted_data[i * batch_size : (i + 1) * batch_size])\n",
    "    inputs = np.asarray(inputs)\n",
    "    expected_outputs = np.asarray([delighted_data[mesh] for mesh in meshes])\n",
    "    return expected_outputs, inputs\n",
    "\n",
    "def restore(sess, checkpoint_file):\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, checkpoint_file)\n",
    "    \n",
    "def save(sess, checkpoint_file):\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-19T07:03:11.676980",
     "start_time": "2017-05-19T00:03:06.661742-07:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "gen_hyperparams = {\n",
    "    'learning_rate' : 1e-4,\n",
    "    'gdl_alpha' : 1,\n",
    "    'lambda_l2' : 0.3,\n",
    "    'lambda_gdl' : 0,\n",
    "    'lambda_adv' : 5\n",
    "}\n",
    "generator = GeneratorNetwork(sess, train_input_dimensions, test_input_dimensions, gen_hyperparams,\n",
    "                             input_residual=False,\n",
    "                             alpha_mask_loss=True)\n",
    "discriminator = DiscriminatorNetwork(sess, train_input_dimensions, learning_rate=1e-4)\n",
    "generator.init_network(discriminator)\n",
    "discriminator.init_network(generator)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-19T06:55:01.051289",
     "start_time": "2017-05-18T23:46:09.828101-07:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "n_samples = len(train_lighted_data)\n",
    "batch_size = 50\n",
    "display_step = 1\n",
    "iters_per_epoch = n_samples // batch_size\n",
    "summary_interval = iters_per_epoch\n",
    "start_epoch = 0\n",
    "\n",
    "summary_writer = tf.summary.FileWriter('summaries/test5', graph=sess.graph)\n",
    "mean_gen_losses = []\n",
    "mean_disc_losses = []\n",
    "for epoch in range(start_epoch, start_epoch + epochs):\n",
    "    total_gen_loss = 0\n",
    "    total_disc_loss = 0\n",
    "    for i in range(iters_per_epoch):\n",
    "        expected_outputs, inputs, alphas = get_train_batch(delighted_data, train_lighted_data, batch_size, i)\n",
    "        gen_loss, gen_summaries = generator.fit_batch(inputs, expected_outputs, alphas)\n",
    "        disc_loss, disc_summaries = discriminator.fit_batch(inputs, expected_outputs)\n",
    "        if (i + 1) % summary_interval == 0:\n",
    "            step = epoch * n_samples + (i + 1) * batch_size\n",
    "            summary_writer.add_summary(gen_summaries, step)\n",
    "            summary_writer.add_summary(disc_summaries, step)\n",
    "        total_gen_loss += gen_loss\n",
    "        total_disc_loss += disc_loss\n",
    "    mean_gen_loss = total_gen_loss / iters_per_epoch\n",
    "    mean_disc_loss = total_disc_loss / iters_per_epoch\n",
    "    mean_gen_losses.append(mean_gen_loss)\n",
    "    mean_disc_losses.append(mean_disc_loss)\n",
    "    if (epoch + 1) % display_step == 0:\n",
    "        print('epoch %s: gen_loss=%.4f, disc_loss=%.4f' % (epoch + 1, mean_gen_loss, mean_disc_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-19T06:55:48.339370",
     "start_time": "2017-05-18T23:55:46.549071-07:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label, img = test_lighted_data[2]\n",
    "def show(img):\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "print('lighted')\n",
    "show(img)\n",
    "print('delighted')\n",
    "show(delighted_data[label])\n",
    "print('predicted')\n",
    "show(float_to_uint8(generator.predict(np.array([img])))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
