{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-18T21:03:56.876743",
     "start_time": "2017-05-18T14:03:37.861976-07:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%aimport util\n",
    "from util import *\n",
    "%autoreload 1\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-18T21:04:52.810902",
     "start_time": "2017-05-18T14:03:56.876743-07:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delighted_dirs, lighted_dirs = scan_lighted_delighted('data/')\n",
    "delighted_data, lighted_data = load_dataset(delighted_dirs, lighted_dirs)\n",
    "test_meshes = ['Mesh_000003', 'Mesh_000006']\n",
    "train_lighted_data = [(label, arr) for label, arr in lighted_data if label not in test_meshes]\n",
    "test_lighted_data = [(label, arr) for label, arr in lighted_data if label in test_meshes]\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(train_lighted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-18T21:04:52.841640",
     "start_time": "2017-05-18T14:04:52.816622-07:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GEN_ENCODER = 'gen_encoder'\n",
    "GEN_MID = 'gen_mid'\n",
    "GEN_DECODER = 'gen_decoder'\n",
    "DISC_CONV = 'disc_conv'\n",
    "DISC_FC = 'disc_fc'\n",
    "\n",
    "def get_dimension_name(stage, layer_num):\n",
    "    return 'dims_' + stage + '_' + str(layer_num)\n",
    "\n",
    "def get_weight_name(stage, layer_num):\n",
    "    return 'w_' + stage + '_' + str(layer_num)\n",
    "    \n",
    "def get_bias_name(stage, layer_num):\n",
    "    return 'b_' + stage + '_' + str(layer_num)\n",
    "\n",
    "FULL_HEIGHT = 1024\n",
    "TRAIN_HEIGHT = 32\n",
    "TEST_HEIGHT = FULL_HEIGHT\n",
    "\n",
    "FILTER_SIZE = 3\n",
    "INPUT_DEPTH = 3\n",
    "\n",
    "# output depths of the layers\n",
    "GEN_ENCODER_DEPTHS = [4, 8, 16]\n",
    "GEN_MID_DEPTH = GEN_ENCODER_DEPTHS[-1]\n",
    "GEN_DECODER_DEPTHS = [8, 4, 3]\n",
    "\n",
    "DISC_CONV_DEPTHS = [4, 8, 16]\n",
    "DISC_FC_SIZES = [256, 128, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-18T21:04:53.153986",
     "start_time": "2017-05-18T14:04:52.863654-07:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_layer_dimensions = {}\n",
    "prev_depth = INPUT_DEPTH\n",
    "for i, output_depth in enumerate(GEN_ENCODER_DEPTHS):\n",
    "    weight_name = get_dimension_name(GEN_ENCODER, i)\n",
    "    stride = 1 if i == 0 else 2 # stride 1 for the first conv layer only\n",
    "    gen_layer_dimensions[weight_name] = {\n",
    "        'input_depth' : prev_depth,\n",
    "        'output_depth' : output_depth,\n",
    "        'stride' : stride,\n",
    "        'filter_size' : FILTER_SIZE\n",
    "    }\n",
    "    prev_depth = output_depth\n",
    "\n",
    "for i in range(1):\n",
    "    weight_name = get_dimension_name(GEN_MID, i)\n",
    "    gen_layer_dimensions[weight_name] = {\n",
    "        'input_depth' : GEN_MID_DEPTH,\n",
    "        'output_depth' : GEN_MID_DEPTH,\n",
    "        'stride' : 1,\n",
    "        'filter_size' : FILTER_SIZE\n",
    "    }\n",
    "\n",
    "prev_depth = GEN_MID_DEPTH\n",
    "for i, output_depth in enumerate(GEN_DECODER_DEPTHS):\n",
    "    if i < 2:\n",
    "        prev_encoder_depth = GEN_ENCODER_DEPTHS[2 - i]\n",
    "        input_depth = prev_depth + prev_encoder_depth\n",
    "    else: # 3rd layer doesn't have concatenation\n",
    "        input_depth = prev_depth\n",
    "    weight_name = get_dimension_name(GEN_DECODER, i)\n",
    "    gen_layer_dimensions[weight_name] = {\n",
    "        'input_depth' : input_depth,\n",
    "        'output_depth' : output_depth,\n",
    "        'stride' : 1,\n",
    "        'filter_size' : FILTER_SIZE\n",
    "    }\n",
    "    prev_depth = output_depth\n",
    "\n",
    "disc_layer_dimensions = {}\n",
    "prev_depth = INPUT_DEPTH\n",
    "prev_height = TRAIN_HEIGHT\n",
    "for i, output_depth in enumerate(DISC_CONV_DEPTHS):\n",
    "    weight_name = get_dimension_name(DISC_CONV, i)\n",
    "    disc_layer_dimensions[weight_name] = {\n",
    "        'input_depth' : prev_depth,\n",
    "        'output_depth' : output_depth,\n",
    "        'stride' : stride,\n",
    "        'filter_size' : FILTER_SIZE\n",
    "    }\n",
    "    prev_depth = output_depth\n",
    "    prev_height = prev_height // stride\n",
    "\n",
    "prev_size = prev_depth * prev_height ** 2\n",
    "for i, output_size in enumerate(DISC_FC_SIZES):\n",
    "    weight_name = get_dimension_name(DISC_FC, i)\n",
    "    disc_layer_dimensions[weight_name] = {\n",
    "        'input_size' : prev_size,\n",
    "        'output_size' : output_size,\n",
    "    }\n",
    "    prev_size = output_size\n",
    "\n",
    "train_input_dimensions = (TRAIN_HEIGHT, TRAIN_HEIGHT, 3)\n",
    "test_input_dimensions = (TEST_HEIGHT, TEST_HEIGHT, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-18T21:53:56.637543",
     "start_time": "2017-05-18T14:53:56.567490-07:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc(x, output_depth, name, activation=tf.nn.relu):\n",
    "    input_depth = int(x.get_shape()[-1])\n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.get_variable('W', shape=[input_depth, output_depth], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.get_variable('b', [output_depth], initializer=tf.zeros_initializer())\n",
    "        return activation(tf.matmul(x, W) + b)\n",
    "    \n",
    "def conv2d(x, output_depth, name, filter_size=3, stride=1, padding='SAME'):\n",
    "    input_depth = int(x.get_shape()[-1])\n",
    "\n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.get_variable('W',\n",
    "                            shape=[filter_size, filter_size, input_depth, output_depth],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.get_variable('b', shape=[output_depth], initializer=tf.zeros_initializer())\n",
    "        conv_output = tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding=padding) + b\n",
    "        return tf.nn.relu(conv_output)\n",
    "\n",
    "def deconv2d(x, output_depth, name, filter_size=3, stride=1, padding='SAME'):\n",
    "    _, old_height, old_width, input_depth = x.get_shape().as_list()\n",
    "    x = conv2d(x, output_depth, name, filter_size=filter_size, stride=stride, padding=padding)\n",
    "    with tf.variable_scope(name):\n",
    "        # bilinear interpolation upsampling\n",
    "        scale = 2\n",
    "        new_height = old_height * scale\n",
    "        new_width = old_width * scale\n",
    "        return tf.image.resize_images(x, [new_height, new_width], method=tf.image.ResizeMethod.BILINEAR)\n",
    "\n",
    "def gradient_difference_loss(expected, predicted, alpha):\n",
    "    pos = tf.constant(np.identity(3), dtype=tf.float32)\n",
    "    filter_x = tf.expand_dims(tf.stack([-pos, pos]), 0)  # [-1, 1]\n",
    "    filter_y = tf.stack([tf.expand_dims(pos, 0), tf.expand_dims(-pos, 0)])  # [[1], [-1]]\n",
    "\n",
    "    predicted_dx = tf.abs(tf.nn.conv2d(predicted, filter_x, [1, 1, 1, 1], padding='SAME'))\n",
    "    predicted_dy = tf.abs(tf.nn.conv2d(predicted, filter_y, [1, 1, 1, 1], padding='SAME'))\n",
    "    expected_dx = tf.abs(tf.nn.conv2d(expected, filter_x, [1, 1, 1, 1], padding='SAME'))\n",
    "    expected_dy = tf.abs(tf.nn.conv2d(expected, filter_y, [1, 1, 1, 1], padding='SAME'))\n",
    "\n",
    "    grad_diff_x = tf.abs(expected_dx - predicted_dx)\n",
    "    grad_diff_y = tf.abs(expected_dy - predicted_dy)\n",
    "\n",
    "    return tf.reduce_mean(grad_diff_x ** alpha + grad_diff_y ** alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-18T21:54:00.780739",
     "start_time": "2017-05-18T14:54:00.545105-07:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GeneratorNetwork(object):\n",
    "    def __init__(self, session, train_input_dimensions, test_input_dimensions, dimensions, hyperparameters=None):\n",
    "        self.sess = session\n",
    "        self.train_input_dimensions = train_input_dimensions\n",
    "        self.test_input_dimensions = test_input_dimensions\n",
    "        self.dimensions = dimensions\n",
    "        if not hyperparameters:\n",
    "            hyperparameters = {\n",
    "                'learning_rate' : 1e-3,\n",
    "                'gdl_alpha' : 1,\n",
    "                'lambda_l2' : 1,\n",
    "                'lambda_gdl' : 1,\n",
    "                'lambda_adv' : 1\n",
    "            }\n",
    "        self.hyperparams = hyperparameters\n",
    "        self.train_variables = []\n",
    "        self.has_defined_layers = False\n",
    "    \n",
    "    def init_network(self, discriminator):\n",
    "        self.train_input = tf.placeholder(tf.float32, shape=[None,] + list(self.train_input_dimensions))\n",
    "        self.test_input = tf.placeholder(tf.float32, shape=[None,] + list(self.test_input_dimensions))\n",
    "        self.expected_output = tf.placeholder(tf.float32, shape=[None,] + list(self.train_input_dimensions))\n",
    "        \n",
    "        train_output = self.get_output_tensor(self.train_input)\n",
    "        self.test_output = self.get_output_tensor(self.test_input)\n",
    "        \n",
    "        l2_loss = tf.reduce_mean(tf.squared_difference(train_output, self.expected_output))\n",
    "        gdl_loss = gradient_difference_loss(train_output, self.expected_output, self.hyperparams['gdl_alpha'])\n",
    "        adv_loss = -tf.reduce_mean(tf.log(discriminator.get_output_tensor(train_output)))\n",
    "        self.loss = self.hyperparams['lambda_l2'] * l2_loss \\\n",
    "                  + self.hyperparams['lambda_gdl'] * gdl_loss \\\n",
    "                  + self.hyperparams['lambda_adv'] * adv_loss\n",
    "        self.opt = tf.train.AdamOptimizer(learning_rate=self.hyperparams['learning_rate']).minimize(self.loss, var_list=self.train_variables)\n",
    "    \n",
    "    def get_output_tensor(self, input):\n",
    "        with tf.variable_scope('generator', reuse=self.has_defined_layers):\n",
    "            encoder0 = conv2d(input, 4, 'encoder_0')\n",
    "            encoder1 = conv2d(encoder0, 8, 'encoder_1', stride=2)\n",
    "            encoder2 = conv2d(encoder1, 16, 'encoder_2', stride=2)\n",
    "            mid0 = conv2d(encoder2, 16, 'mid_0')\n",
    "            decoder0 = deconv2d(tf.concat([mid0, encoder2], axis=3), 8, 'decoder_0')\n",
    "            decoder1 = deconv2d(tf.concat([decoder0, encoder1], axis=3), 4, 'decoder_1')\n",
    "            output = conv2d(decoder1, 3, 'decoder_2')\n",
    "        if not self.has_defined_layers:\n",
    "            self.train_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator')\n",
    "            self.has_defined_layers = True\n",
    "        return output\n",
    "    \n",
    "    def fit_batch(self, inputs, expected_outputs):\n",
    "        _, loss = self.sess.run((self.opt, self.loss), feed_dict={self.train_input : inputs, self.expected_output : expected_outputs})\n",
    "        return loss        \n",
    "                \n",
    "    def predict(self, inputs):\n",
    "        return self.sess.run(self.test_output, feed_dict={self.test_input : inputs})\n",
    "    \n",
    "class DiscriminatorNetwork(object):\n",
    "    def __init__(self, session, train_input_dimensions, dimensions, learning_rate=1e-3):\n",
    "        self.sess = session\n",
    "        self.train_input_dimensions = train_input_dimensions\n",
    "        self.dimensions = dimensions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_variables = []\n",
    "        self.has_defined_layers = False\n",
    "    \n",
    "    def init_network(self, generator):\n",
    "        self.lighted_input = tf.placeholder(tf.float32, shape=[None,] + list(self.train_input_dimensions))\n",
    "        self.delighted_input = tf.placeholder(tf.float32, shape=[None,] + list(self.train_input_dimensions))\n",
    "        real_input = self.delighted_input\n",
    "        fake_input = generator.get_output_tensor(self.lighted_input)\n",
    "        \n",
    "        predicted_real = self.get_output_tensor(real_input)\n",
    "        predicted_fake = self.get_output_tensor(fake_input)\n",
    "        \n",
    "        self.loss = -tf.reduce_mean(tf.log(predicted_real) + tf.log(1 - predicted_fake))\n",
    "        self.opt = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss, var_list=self.train_variables)\n",
    "        \n",
    "    def get_output_tensor(self, input):\n",
    "        '''\n",
    "        Given an input tensor / placeholder, perform convs then FCs to get the probability that input is real\n",
    "        '''\n",
    "        with tf.variable_scope('discriminator', reuse=self.has_defined_layers):\n",
    "            conv0 = conv2d(input, 4, 'conv_0', stride=2)\n",
    "            conv1 = conv2d(conv0, 8, 'conv_1', stride=2)\n",
    "            conv2 = conv2d(conv1, 16, 'conv_2', stride=2)\n",
    "            fc_input = tf.contrib.layers.flatten(conv2)\n",
    "            fc_0 = fc(fc_input, 256, 'fc_0')\n",
    "            fc_1 = fc(fc_0, 128, 'fc_1')\n",
    "            output = fc(fc_1, 1, 'fc_2', activation=tf.nn.sigmoid)\n",
    "        if not self.has_defined_layers:\n",
    "            self.train_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='discriminator')\n",
    "            self.has_defined_layers = True\n",
    "        return output\n",
    "        \n",
    "    def fit_batch(self, lighted_inputs, delighted_inputs):\n",
    "        _, loss = self.sess.run((self.opt, self.loss), feed_dict={ self.lighted_input : lighted_inputs, self.delighted_input : delighted_inputs })\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-18T21:46:40.070058",
     "start_time": "2017-05-18T14:46:40.024027-07:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_batch(delighted_data, lighted_data, batch_size, i):\n",
    "    inputs = []\n",
    "    expected_outputs = []\n",
    "    for i, (mesh, input) in enumerate(lighted_data[i * batch_size : (i + 1) * batch_size]):\n",
    "        while True:\n",
    "            start_x, start_y = np.random.randint(0, FULL_HEIGHT - TRAIN_HEIGHT + 1, size=(2,))\n",
    "            sample = input[start_x : start_x + TRAIN_HEIGHT, start_y : start_y + TRAIN_HEIGHT]\n",
    "            if np.sum(sample > 0) > 0.1 * TRAIN_HEIGHT ** 2:\n",
    "                rotate_rand = np.random.randint(4)\n",
    "                flip_rand = np.random.randint(0, 1, size=1)[0]\n",
    "                sample = np.rot90(sample, rotate_rand)\n",
    "                delighted = delighted_data[mesh][start_x : start_x + TRAIN_HEIGHT, start_y : start_y + TRAIN_HEIGHT]\n",
    "                if flip_rand:\n",
    "                    sample = np.flip(sample, axis=0)\n",
    "                    delighted = np.flip(delighted, axis=0)\n",
    "                inputs.append(sample)\n",
    "                expected_outputs.append(delighted)\n",
    "                break\n",
    "    return np.asarray(expected_outputs), np.asarray(inputs)\n",
    "\n",
    "def get_test_batch(delighted_data, lighted_data, batch_size, i):\n",
    "    meshes, inputs = zip(*lighted_data[i * batch_size : (i + 1) * batch_size])\n",
    "    inputs = np.asarray(inputs)\n",
    "    expected_outputs = np.asarray([delighted_data[mesh] for mesh in meshes])\n",
    "    return expected_outputs, inputs\n",
    "\n",
    "def restore(sess, checkpoint_file):\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, checkpoint_file)\n",
    "    \n",
    "def save(sess, checkpoint_file):\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-18T21:54:07.990547",
     "start_time": "2017-05-18T14:54:04.241647-07:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "gen_hyperparams = {\n",
    "    'learning_rate' : 1e-3,\n",
    "    'gdl_alpha' : 1,\n",
    "    'lambda_l2' : 1,\n",
    "    'lambda_gdl' : 0.2,\n",
    "    'lambda_adv' : 0.1\n",
    "}\n",
    "generator = GeneratorNetwork(sess, train_input_dimensions, test_input_dimensions, gen_layer_dimensions, gen_hyperparams)\n",
    "discriminator = DiscriminatorNetwork(sess, train_input_dimensions, disc_layer_dimensions)\n",
    "generator.init_network(discriminator)\n",
    "discriminator.init_network(generator)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "n_samples = len(train_lighted_data)\n",
    "batch_size = 50\n",
    "display_step = 1\n",
    "\n",
    "mean_gen_losses = []\n",
    "mean_disc_losses = []\n",
    "for epoch in range(epochs):\n",
    "    total_iter = n_samples // batch_size\n",
    "    total_gen_loss = 0\n",
    "    total_disc_loss = 0\n",
    "    for i in range(total_iter):\n",
    "        expected_outputs, inputs = get_train_batch(delighted_data, train_lighted_data, batch_size, i)\n",
    "        gen_loss = generator.fit_batch(inputs, expected_outputs)\n",
    "        disc_loss = discriminator.fit_batch(inputs, expected_outputs)\n",
    "        total_gen_loss += gen_loss\n",
    "        total_disc_loss += disc_loss\n",
    "    mean_gen_loss = total_gen_loss / total_iter\n",
    "    mean_disc_loss = total_disc_loss / total_iter\n",
    "    mean_gen_losses.append(mean_gen_loss)\n",
    "    mean_disc_losses.append(mean_disc_loss)\n",
    "    if (epoch + 1) % display_step == 0:\n",
    "        print('epoch %s: gen_loss=%.4f, disc_loss=%.4f' % (epoch + 1, mean_gen_loss, mean_disc_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label, img = test_lighted_data[0]\n",
    "def show(img):\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "print('lighted')\n",
    "show(img)\n",
    "print('delighted')\n",
    "show(delighted_data[label])\n",
    "print('predicted')\n",
    "show(float_to_uint8(generator.predict(np.array([img])))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
