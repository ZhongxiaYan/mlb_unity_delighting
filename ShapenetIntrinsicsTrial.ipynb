{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T20:11:41.405438",
     "start_time": "2017-04-25T13:11:41.235675-07:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import util\n",
    "from util import *\n",
    "from PIL import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T20:13:20.803706",
     "start_time": "2017-04-25T13:11:54.639520-07:00"
    },
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# code to read weights\n",
    "import re\n",
    "\n",
    "params = []\n",
    "end_re = re.compile(r'^\\[torch.CudaTensor of size (\\w+)\\]')\n",
    "index_re = re.compile(r'^\\((\\d+),(\\d+)')\n",
    "with open('model_weights_all.txt') as f:\n",
    "    current = []\n",
    "    linenum = 1\n",
    "    for line in f:\n",
    "        end = end_re.match(line)\n",
    "        if end:\n",
    "            end_dims = tuple(map(int, end.group(1).split('x')))\n",
    "            if len(end_dims) == 1:\n",
    "                if current[0].startswith('0.01 *'):\n",
    "                    current = current[1:]\n",
    "                array = np.array(tuple(map(float, current)))\n",
    "                assert(len(array) == end_dims[0])\n",
    "            else:\n",
    "                array = np.zeros(end_dims)\n",
    "                i = 0\n",
    "                while i < len(current):\n",
    "                    match = index_re.match(current[i])\n",
    "                    first_index = int(match.group(1)) - 1\n",
    "                    second_index = int(match.group(2)) - 1\n",
    "                    for j in range(3):\n",
    "                        array[first_index][second_index][j] = np.array(tuple(map(float, current[i + 1 + j].lstrip().split())))\n",
    "                    i += 5\n",
    "            params.append(array)\n",
    "            current = []\n",
    "            continue\n",
    "        current.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T21:22:04.348955",
     "start_time": "2017-04-25T14:22:04.208280-07:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_values = {}\n",
    "layer_index = 0\n",
    "for encoder_layer in range(6):\n",
    "    weight_name = get_weight_name(ENCODER, encoder_layer)\n",
    "    bias_name = get_bias_name(ENCODER, encoder_layer)\n",
    "    param_values[weight_name] = params[layer_index]\n",
    "    param_values[bias_name] = params[layer_index + 1]\n",
    "    layer_index += 4\n",
    "\n",
    "for branch in range(3):\n",
    "    for mid_layer in range(4):\n",
    "        weight_name = get_weight_name(MID, (branch, mid_layer))\n",
    "        bias_name = get_bias_name(MID, (branch, mid_layer))\n",
    "        param_values[weight_name] = params[layer_index]\n",
    "        param_values[bias_name] = params[layer_index + 1]\n",
    "        layer_index += 4\n",
    "\n",
    "for decoder_layer in range(5):\n",
    "    for branch in range(3):\n",
    "        weight_name = get_weight_name(DECODER, (branch, decoder_layer))\n",
    "        bias_name = get_bias_name(DECODER, (branch, decoder_layer))\n",
    "        param_values[weight_name] = params[layer_index]\n",
    "        param_values[bias_name] = params[layer_index + 1]\n",
    "        layer_index += 4\n",
    "        \n",
    "for decoder_layer in range(5, 7):\n",
    "    weight_name = get_weight_name(DECODER, decoder_layer)\n",
    "    bias_name = get_bias_name(DECODER, decoder_layer)\n",
    "    param_values[weight_name] = params[layer_index]\n",
    "    param_values[bias_name] = params[layer_index + 1]\n",
    "    layer_index += 4\n",
    "param_values_32 = { name : np.moveaxis(value.astype(np.float32), (0, 1, 2, 3), (3, 2, 0, 1)) if len(value.shape) == 4 else value.astype(np.float32) for name, value in param_values.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-24T21:30:25.643509",
     "start_time": "2017-04-24T14:29:45.624356-07:00"
    },
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    delighted_data = {}\n",
    "    for mesh, mesh_dir in delighted_dirs.items():\n",
    "        mesh_im = Image.open(mesh_dir + '%s_HD_BC.tga' % mesh, 'r')\n",
    "        mesh_im = mesh_im.resize((256, 256), Image.ANTIALIAS)\n",
    "        mesh_array = zero_rgb(np.array(mesh_im))\n",
    "        mesh_array = mesh_array[:,:,:3]        \n",
    "        delighted_data[mesh] = np.asarray([mesh_array])\n",
    "\n",
    "    lighted_data = []\n",
    "    pyramid_depth = 8\n",
    "    reduction_factor = 2\n",
    "    for t, mesh, v, mesh_dir in lighted_dirs:\n",
    "        lit_file = '%s%s_%s_%s_Lit.tga' % (mesh_dir, t, mesh, v)\n",
    "        if not os.path.exists(lit_file):\n",
    "            continue\n",
    "        mesh_im = Image.open(lit_file, 'r')\n",
    "        # resize according to shapenet\n",
    "        mesh_im = mesh_im.resize((256, 256), Image.ANTIALIAS)\n",
    "        meshes = np.asarray([np.array(mesh_im)[:,:,:3]])\n",
    "        lighted_data.append((mesh, meshes))\n",
    "        \n",
    "    return delighted_data, lighted_data\n",
    "\n",
    "#unused method to scale from 0 to 1~ish\n",
    "def preproc(unclean_batch_x):\n",
    "    \"\"\"Convert values to range 0-1\"\"\"\n",
    "    temp_batch = unclean_batch_x / unclean_batch_x.max()\n",
    "    return temp_batch\n",
    "\n",
    "def shuffle(Xtrain, ytrain):\n",
    "    stacked = np.column_stack((Xtrain,ytrain))\n",
    "    np.random.shuffle(stacked)\n",
    "    return stacked[:,:Xtrain.shape[1]], stacked[:,Xtrain.shape[1]]\n",
    "\n",
    "date_dir = ''\n",
    "delighted_dirs, lighted_dirs = scan_lighted_delighted(date_dir)\n",
    "\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "delighted_data, lighted_data = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T19:10:57.508925",
     "start_time": "2017-04-25T12:10:57.485422-07:00"
    },
    "code_folding": [
     2,
     15
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T18:50:16.212064",
     "start_time": "2017-04-25T11:50:16.127040-07:00"
    },
    "code_folding": [
     0,
     7,
     20
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_kernel_size(factor):\n",
    "    \"\"\"\n",
    "    Find the kernel size given the desired factor of upsampling.\n",
    "    \"\"\"\n",
    "    return 2 * factor - factor % 2\n",
    "\n",
    "\n",
    "def upsample_filt(size):\n",
    "    \"\"\"\n",
    "    Make a 2D bilinear kernel suitable for upsampling of the given (h, w) size.\n",
    "    \"\"\"\n",
    "    factor = (size + 1) // 2\n",
    "    if size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = np.ogrid[:size, :size]\n",
    "    return (1 - abs(og[0] - center) / factor) * \\\n",
    "           (1 - abs(og[1] - center) / factor)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "# wrapper for applying spatial conv, batchnorm, reLU\n",
    "def conv2d(x, W, b, stride):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding='SAME') + b\n",
    "    # Batch Norm\n",
    "    x = tf.contrib.layers.batch_norm(x, center=True, scale=True)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def deconv2d(x, W, b, stride):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding='SAME') + b\n",
    "    # Batch Norm\n",
    "    x = tf.contrib.layers.batch_norm(x, center=True, scale=True)\n",
    "    x = tf.nn.relu(x)\n",
    "    \n",
    "    # bilinear interpolation upsampling\n",
    "    old_height, old_width = x.get_shape().as_list()[1 : 3]\n",
    "    scale = 2\n",
    "    new_height = old_height * scale\n",
    "    new_width = old_width * scale\n",
    "    return tf.image.resize_images(x, [new_height, new_width], method=tf.image.ResizeMethod.BILINEAR)\n",
    "\n",
    "ENCODER = 'encoder'\n",
    "MID = 'mid'\n",
    "DECODER = 'decoder'\n",
    "\n",
    "def get_dimension_name(stage, layer_num):\n",
    "    return 'dims_' + stage + '_' + str(layer_num)\n",
    "\n",
    "def get_weight_name(stage, layer_num):\n",
    "    return 'w_' + stage + '_' + str(layer_num)\n",
    "    \n",
    "def get_bias_name(stage, layer_num):\n",
    "    return 'b_' + stage + '_' + str(layer_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T19:01:22.391000",
     "start_time": "2017-04-25T12:01:22.294476-07:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "# more parameters\n",
    "FILTER_SIZE = 3\n",
    "INPUT_DEPTH = 3\n",
    "\n",
    "# output depths of the layers\n",
    "ENCODER_DEPTHS = [16, 32, 64, 128, 256, 256]\n",
    "MID_DEPTH = ENCODER_DEPTHS[-1]\n",
    "DECODER_DEPTHS = [256, 128, 64, 32, 16, 16, 3]\n",
    "\n",
    "# defines the sizes for each of the conv / deconv layers\n",
    "layer_dimensions = {}\n",
    "prev_depth = INPUT_DEPTH\n",
    "for i, output_depth in enumerate(ENCODER_DEPTHS):\n",
    "    weight_name = get_dimension_name(ENCODER, i)\n",
    "    stride = 1 if i == 0 else 2 # stride 1 for the first conv layer only\n",
    "    layer_dimensions[weight_name] = {\n",
    "        'input_depth' : prev_depth,\n",
    "        'output_depth' : output_depth,\n",
    "        'stride' : stride,\n",
    "        'filter_size' : FILTER_SIZE\n",
    "    }\n",
    "    prev_depth = output_depth\n",
    "\n",
    "for j in range(3):\n",
    "    for i in range(4):\n",
    "        weight_name = get_dimension_name(MID, (j, i))\n",
    "        layer_dimensions[weight_name] = {\n",
    "            'input_depth' : MID_DEPTH,\n",
    "            'output_depth' : MID_DEPTH,\n",
    "            'stride' : 1,\n",
    "            'filter_size' : FILTER_SIZE\n",
    "        }\n",
    "\n",
    "prev_depth = MID_DEPTH\n",
    "for i, output_depth in enumerate(DECODER_DEPTHS):\n",
    "    # the ith deconv layer's input is the concatenation of the previous output and the output of (5-i)th encoder conv layer\n",
    "    # so the depth is the sum of the depths\n",
    "    # the last two layers in this loop (5th and 6th layer) are conv layers, not deconv\n",
    "    if i < 6:\n",
    "        prev_encoder_depth = ENCODER_DEPTHS[5 - i]\n",
    "        input_depth = 3 * prev_depth + prev_encoder_depth\n",
    "    else: # 6th layer doesn't have concatenation\n",
    "        input_depth = prev_depth\n",
    "    value = {\n",
    "        'input_depth' : input_depth,\n",
    "        'output_depth' : output_depth,\n",
    "        'stride' : 1,\n",
    "        'filter_size' : FILTER_SIZE\n",
    "    }\n",
    "    if i < 5:\n",
    "        for j in range(3):\n",
    "            weight_name = get_dimension_name(DECODER, (j, i))\n",
    "            layer_dimensions[weight_name] = value\n",
    "    else:\n",
    "        layer_dimensions[get_dimension_name(DECODER, i)] = value\n",
    "    prev_depth = output_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T20:54:20.403091",
     "start_time": "2017-04-25T13:54:20.189359-07:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class IntrinsicNetwork(object):\n",
    "    def __init__(self, input_dimensions, dimensions, param_values, learning_rate=1e-3):\n",
    "        graph = tf.Graph()\n",
    "        self.sess = tf.InteractiveSession(graph=graph)\n",
    "        \n",
    "        self.dimensions = dimensions\n",
    "        self.params = {}\n",
    "        self.param_values = param_values\n",
    "        \n",
    "        # in the paper each input sample is 256x256x3\n",
    "        self.inp = tf.placeholder(tf.float32, shape=[None,] + list(input_dimensions))\n",
    "        self.expected_output = tf.placeholder(tf.float32, shape=[None,] + list(input_dimensions))\n",
    "        \n",
    "        encoder_layers = self.get_encoder_layers(self.inp)\n",
    "        mid_outputs = self.get_mid_outputs(encoder_layers[-1])\n",
    "        self.output = self.get_decoder_output(mid_outputs, encoder_layers)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.squared_difference(self.output, self.expected_output))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.loss)\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "#         show_graph(graph.as_graph_def())\n",
    "    \n",
    "    def create_weights(self, stage, layer_num):\n",
    "        dims = self.dimensions[get_dimension_name(stage, layer_num)]\n",
    "        input_depth, output_depth, stride, filter_size = \\\n",
    "            (dims[x] for x in ['input_depth', 'output_depth', 'stride', 'filter_size'])\n",
    "        weight_name = get_weight_name(stage, layer_num)\n",
    "        bias_name = get_bias_name(stage, layer_num)\n",
    "        W = tf.Variable(self.param_values[weight_name])\n",
    "        b = tf.Variable(self.param_values[bias_name])\n",
    "#         W = tf.Variable(tf.random_normal([filter_size, filter_size, input_depth, output_depth]))\n",
    "#         b = tf.Variable(tf.random_normal([output_depth]))\n",
    "        self.params[weight_name] = W\n",
    "        self.params[bias_name] = b\n",
    "        return W, b, stride\n",
    "        \n",
    "    def get_encoder_layers(self, input):\n",
    "        prev = input\n",
    "        encoder_layers = []\n",
    "        # encoder layers 0 to 5\n",
    "        for layer_num in range(6):\n",
    "            W, b, stride = self.create_weights(ENCODER, layer_num)\n",
    "            prev = conv2d(prev, W, b, stride)\n",
    "            encoder_layers.append(prev)\n",
    "        return encoder_layers\n",
    "    \n",
    "    def get_mid_outputs(self, encoder_output):\n",
    "        mid_outputs = []\n",
    "        for branch in range(3):\n",
    "            prev = encoder_output\n",
    "            for layer_num in range(4):\n",
    "                W, b, stride = self.create_weights(MID, (branch, layer_num))\n",
    "                prev = conv2d(prev, W, b, stride)\n",
    "            mid_outputs.append(prev)\n",
    "        return mid_outputs\n",
    "    \n",
    "    def get_decoder_output(self, mid_outputs, encoder_layers):\n",
    "        prevs = mid_outputs\n",
    "        for layer_num in range(5):\n",
    "            encoder_layer_input = encoder_layers[5 - layer_num]\n",
    "            prev = tf.concat(prevs + [encoder_layer_input], axis=3)\n",
    "            new_branches = []\n",
    "            for branch in range(3):\n",
    "                W, b, stride = self.create_weights(DECODER, (branch, layer_num))\n",
    "                # concatenate with encoder layer for layers 0 to 5                \n",
    "                new_branch = deconv2d(prev, W, b, stride)\n",
    "                new_branches.append(new_branch)\n",
    "            prevs = new_branches\n",
    "        for layer_num in range(5, 7):\n",
    "            if layer_num == 5:\n",
    "                merged = tf.concat(prevs + [encoder_layers[0]], axis=3)\n",
    "            W, b, stride = self.create_weights(DECODER, layer_num)\n",
    "            merged = conv2d(merged, W, b, stride)        \n",
    "        return merged\n",
    "    \n",
    "    def fit_batch(self, inputs, output):\n",
    "        _, loss = self.sess.run((self.optimizer, self.loss), feed_dict={self.inp : inputs, self.expected_output : output})\n",
    "        return loss\n",
    "    \n",
    "    def train(self, lighted_inputs, delighted_data, epochs, batch_size=1, display_step=5):\n",
    "        random.shuffle(lighted_inputs)\n",
    "        n_samples = len(lighted_inputs)\n",
    "        mean_losses = []\n",
    "        for epoch in range(epochs):\n",
    "            total_iter = n_samples // batch_size\n",
    "            total_loss = 0\n",
    "            for i in range(total_iter):\n",
    "                mesh, inputs = lighted_inputs[i * batch_size]\n",
    "                loss = self.fit_batch(inputs, delighted_data[mesh])\n",
    "                total_loss += loss\n",
    "            mean_loss = total_loss / total_iter\n",
    "            mean_losses.append(mean_loss)\n",
    "            if (epoch + 1) % display_step == 0:\n",
    "                print('epoch %s: loss=%.4f' % (epoch + 1, mean_loss))\n",
    "                \n",
    "    def predict(self, inputs):\n",
    "        return self.sess.run(self.output, feed_dict={self.inp : inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T21:22:48.615748",
     "start_time": "2017-04-25T14:22:16.720998-07:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inet = IntrinsicNetwork((256, 256, INPUT_DEPTH), layer_dimensions, param_values_32)\n",
    "# inet.train(lighted_data, delighted_data, epochs=1, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
