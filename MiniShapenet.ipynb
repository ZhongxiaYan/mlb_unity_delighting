{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import util\n",
    "from util import *\n",
    "from PIL import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    delighted_data = {}\n",
    "    for mesh, mesh_dir in delighted_dirs.items():\n",
    "        mesh_im = Image.open(mesh_dir + '%s_HD_BC.tga' % mesh, 'r')\n",
    "#         mesh_im = mesh_im.resize((256, 256), Image.ANTIALIAS)\n",
    "        mesh_array = zero_rgb(np.array(mesh_im))\n",
    "        mesh_array = mesh_array[:,:,:3]        \n",
    "        delighted_data[mesh] = np.asarray([mesh_array])\n",
    "\n",
    "    lighted_data = []\n",
    "    for t, mesh, v, mesh_dir in lighted_dirs:\n",
    "        lit_file = '%s%s_%s_%s_Lit.tga' % (mesh_dir, t, mesh, v)\n",
    "        if not os.path.exists(lit_file):\n",
    "            continue\n",
    "        mesh_im = Image.open(lit_file, 'r')\n",
    "        # resize according to shapenet\n",
    "#         mesh_im = mesh_im.resize((256, 256), Image.ANTIALIAS)\n",
    "        meshes = np.asarray([np.array(mesh_im)[:,:,:3]])\n",
    "        lighted_data.append((mesh, meshes))\n",
    "        \n",
    "    return delighted_data, lighted_data\n",
    "\n",
    "#unused method to scale from 0 to 1~ish\n",
    "def preproc(unclean_batch_x):\n",
    "    \"\"\"Convert values to range 0-1\"\"\"\n",
    "    temp_batch = unclean_batch_x / unclean_batch_x.max()\n",
    "    return temp_batch\n",
    "\n",
    "def shuffle(Xtrain, ytrain):\n",
    "    stacked = np.column_stack((Xtrain,ytrain))\n",
    "    np.random.shuffle(stacked)\n",
    "    return stacked[:,:Xtrain.shape[1]], stacked[:,Xtrain.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_dir = ''\n",
    "delighted_dirs, lighted_dirs = scan_lighted_delighted(date_dir)\n",
    "\n",
    "delighted_data, lighted_data = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_kernel_size(factor):\n",
    "    \"\"\"\n",
    "    Find the kernel size given the desired factor of upsampling.\n",
    "    \"\"\"\n",
    "    return 2 * factor - factor % 2\n",
    "\n",
    "\n",
    "def upsample_filt(size):\n",
    "    \"\"\"\n",
    "    Make a 2D bilinear kernel suitable for upsampling of the given (h, w) size.\n",
    "    \"\"\"\n",
    "    factor = (size + 1) // 2\n",
    "    if size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = np.ogrid[:size, :size]\n",
    "    return (1 - abs(og[0] - center) / factor) * \\\n",
    "           (1 - abs(og[1] - center) / factor)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "# wrapper for applying spatial conv, batchnorm, reLU\n",
    "def conv2d(x, W, b, stride):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    # Batch Norm\n",
    "    x = tf.contrib.layers.batch_norm(x, center=True, scale=True)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def deconv2d(x, W, b, stride):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    # Batch Norm\n",
    "    x = tf.contrib.layers.batch_norm(x, center=True, scale=True)\n",
    "    x = tf.nn.relu(x)\n",
    "    \n",
    "    # bilinear interpolation upsampling\n",
    "    old_height, old_width = x.get_shape().as_list()[1 : 3]\n",
    "    scale = 2\n",
    "    new_height = old_height * scale\n",
    "    new_width = old_width * scale\n",
    "    return tf.image.resize_images(x, [new_height, new_width], method=tf.image.ResizeMethod.BILINEAR)\n",
    "\n",
    "ENCODER = 'encoder'\n",
    "MID = 'mid'\n",
    "DECODER = 'decoder'\n",
    "\n",
    "def get_dimension_name(stage, layer_num):\n",
    "    return 'dims_' + stage + '_' + str(layer_num)\n",
    "\n",
    "def get_weight_name(stage, layer_num):\n",
    "    return 'w_' + stage + '_' + str(layer_num)\n",
    "    \n",
    "def get_bias_name(stage, layer_num):\n",
    "    return 'b_' + stage + '_' + str(layer_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "# more parameters\n",
    "FILTER_SIZE = 5 #try 7 if 5 doesn't work\n",
    "INPUT_DEPTH = 3 #try 7 if 5 doesn't work\n",
    "\n",
    "# output depths of the layers\n",
    "ENCODER_DEPTHS = [16, 32, 64]\n",
    "MID_DEPTH = ENCODER_DEPTHS[-1]\n",
    "DECODER_DEPTHS = [64, 32, 16, 3]\n",
    "\n",
    "# defines the sizes for each of the conv / deconv layers\n",
    "layer_dimensions = {}\n",
    "prev_depth = INPUT_DEPTH\n",
    "for i, output_depth in enumerate(ENCODER_DEPTHS):\n",
    "    weight_name = get_dimension_name(ENCODER, i)\n",
    "    stride = 1 if i == 0 else 2 # stride 1 for the first conv layer only\n",
    "    layer_dimensions[weight_name] = {\n",
    "        'input_depth' : prev_depth,\n",
    "        'output_depth' : output_depth,\n",
    "        'stride' : stride,\n",
    "        'filter_size' : FILTER_SIZE\n",
    "    }\n",
    "    prev_depth = output_depth\n",
    "\n",
    "for i in range(1):\n",
    "    weight_name = get_dimension_name(MID, i)\n",
    "    layer_dimensions[weight_name] = {\n",
    "        'input_depth' : MID_DEPTH,\n",
    "        'output_depth' : MID_DEPTH,\n",
    "        'stride' : 1,\n",
    "        'filter_size' : FILTER_SIZE\n",
    "    }\n",
    "\n",
    "prev_depth = MID_DEPTH\n",
    "for i, output_depth in enumerate(DECODER_DEPTHS):\n",
    "    # the ith deconv layer's input is the concatenation of the previous output and the output of (2-i)th encoder conv layer\n",
    "    # so the depth is the sum of the depths\n",
    "    # the last two layers in this loop (5th and 6th layer) are conv layers, not deconv\n",
    "    if i < 3:\n",
    "        prev_encoder_depth = ENCODER_DEPTHS[2 - i]\n",
    "        input_depth = prev_depth + prev_encoder_depth\n",
    "    else: # 3rd layer doesn't have concatenation\n",
    "        input_depth = prev_depth\n",
    "    weight_name = get_dimension_name(DECODER, i)\n",
    "    layer_dimensions[weight_name] = {\n",
    "        'input_depth' : input_depth,\n",
    "        'output_depth' : output_depth,\n",
    "        'stride' : 3,\n",
    "        'filter_size' : FILTER_SIZE\n",
    "    }\n",
    "    prev_depth = output_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dims_decoder_0': {'filter_size': 5,\n",
       "  'input_depth': 128,\n",
       "  'output_depth': 64,\n",
       "  'stride': 3},\n",
       " 'dims_decoder_1': {'filter_size': 5,\n",
       "  'input_depth': 96,\n",
       "  'output_depth': 32,\n",
       "  'stride': 3},\n",
       " 'dims_decoder_2': {'filter_size': 5,\n",
       "  'input_depth': 48,\n",
       "  'output_depth': 16,\n",
       "  'stride': 3},\n",
       " 'dims_decoder_3': {'filter_size': 5,\n",
       "  'input_depth': 16,\n",
       "  'output_depth': 3,\n",
       "  'stride': 3},\n",
       " 'dims_encoder_0': {'filter_size': 5,\n",
       "  'input_depth': 3,\n",
       "  'output_depth': 16,\n",
       "  'stride': 1},\n",
       " 'dims_encoder_1': {'filter_size': 5,\n",
       "  'input_depth': 16,\n",
       "  'output_depth': 32,\n",
       "  'stride': 2},\n",
       " 'dims_encoder_2': {'filter_size': 5,\n",
       "  'input_depth': 32,\n",
       "  'output_depth': 64,\n",
       "  'stride': 2},\n",
       " 'dims_mid_0': {'filter_size': 5,\n",
       "  'input_depth': 64,\n",
       "  'output_depth': 64,\n",
       "  'stride': 1}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IntrinsicNetwork(object):\n",
    "    def __init__(self, input_dimensions, dimensions, learning_rate=1e-3):\n",
    "        graph = tf.Graph()\n",
    "        self.sess = tf.InteractiveSession(graph=graph)\n",
    "        \n",
    "        self.dimensions = dimensions\n",
    "        self.params = {}\n",
    "        \n",
    "        #input will be 1024x1024x3\n",
    "        self.inp = tf.placeholder(tf.float32, shape=[None,] + list(input_dimensions))\n",
    "        self.expected_output = tf.placeholder(tf.float32, shape=[None,] + list(input_dimensions))\n",
    "        \n",
    "        encoder_layers = self.get_encoder_layers(self.inp)\n",
    "        mid_output = self.get_mid_output(encoder_layers[-1])\n",
    "        self.output = self.get_decoder_output(mid_output, encoder_layers)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.squared_difference(self.output, self.expected_output))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.loss)\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "    \n",
    "    def create_weights(self, stage, layer_num):\n",
    "        dims = self.dimensions[get_dimension_name(stage, layer_num)]\n",
    "        input_depth, output_depth, stride, filter_size = \\\n",
    "            (dims[x] for x in ['input_depth', 'output_depth', 'stride', 'filter_size'])\n",
    "        W = tf.Variable(tf.random_normal([filter_size, filter_size, input_depth, output_depth]))\n",
    "#         W = tf.get_variable(\"W\", shape=[filter_size, filter_size, input_depth, output_depth], \n",
    "#                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.Variable(tf.random_normal([output_depth]))\n",
    "        self.params[get_weight_name(stage, layer_num)] = W\n",
    "        self.params[get_bias_name(stage, layer_num)] = b\n",
    "        return W, b, stride\n",
    "        \n",
    "    def get_encoder_layers(self, input):\n",
    "        prev = input\n",
    "        encoder_layers = []\n",
    "        # encoder layers 0 to 2\n",
    "        for layer_num in range(3):\n",
    "            W, b, stride = self.create_weights(ENCODER, layer_num)\n",
    "            prev = conv2d(prev, W, b, stride)\n",
    "            encoder_layers.append(prev)\n",
    "        return encoder_layers\n",
    "    \n",
    "    def get_mid_output(self, encoder_output):\n",
    "        prev = encoder_output\n",
    "        for layer_num in range(1):\n",
    "            W, b, stride = self.create_weights(MID, layer_num)\n",
    "            prev = conv2d(prev, W, b, stride)\n",
    "        return prev\n",
    "    \n",
    "#     def get_decoder_output(self, mid_output, encoder_layers):\n",
    "#         prev = mid_output\n",
    "#         for layer_num in range(4):\n",
    "#             W, b, stride = self.create_weights(DECODER, layer_num)\n",
    "#             if layer_num < 3: # concatenate with encoder layer for layers 0 to 2\n",
    "#                 encoder_layer_input = encoder_layers[2 - layer_num]\n",
    "#                 print(prev.shape, encoder_layer_input.shape)\n",
    "#                 prev = tf.concat([prev, encoder_layer_input], axis=3)\n",
    "#             if layer_num < 3: # perform deconv on layers 0 to 2\n",
    "#                 prev = deconv2d(prev, W, b, stride)\n",
    "#             else: # conv on layer 3\n",
    "#                 prev = conv2d(prev, W, b, stride)\n",
    "#         return prev\n",
    "        \n",
    "    def get_decoder_output(self, mid_outputs, encoder_layers):\n",
    "        prev = mid_outputs\n",
    "        for layer_num in range(4):\n",
    "            W, b, stride = self.create_weights(DECODER, layer_num)\n",
    "            if layer_num < 3:\n",
    "                encoder_layer_input = encoder_layers[2 - layer_num]\n",
    "                prev = tf.concat([prev, encoder_layer_input], axis=3)\n",
    "                prev = deconv2d(prev, W, b, stride)\n",
    "            else: \n",
    "                prev = conv2d(prev, W, b, stride)\n",
    "        return prev\n",
    "    \n",
    "    def fit_batch(self, inputs, output):\n",
    "        _, loss = self.sess.run((self.optimizer, self.loss), feed_dict={self.inp : inputs, self.expected_output : output})\n",
    "        return loss\n",
    "    \n",
    "    def train(self, lighted_inputs, delighted_data, epochs, batch_size=1, display_step=5):\n",
    "        random.shuffle(lighted_inputs)\n",
    "        n_samples = len(lighted_inputs)\n",
    "        mean_losses = []\n",
    "        for epoch in range(epochs):\n",
    "            total_iter = n_samples // batch_size\n",
    "            total_loss = 0\n",
    "            for i in range(total_iter):\n",
    "                mesh, inputs = lighted_inputs[i * batch_size]\n",
    "                loss = self.fit_batch(inputs, delighted_data[mesh])\n",
    "                total_loss += loss\n",
    "            mean_loss = total_loss / total_iter\n",
    "            mean_losses.append(mean_loss)\n",
    "            if (epoch + 1) % display_step == 0:\n",
    "                print('epoch %s: loss=%.4f' % (epoch + 1, mean_loss))\n",
    "                \n",
    "    def predict(self, inputs):\n",
    "        return self.sess.run(self.output, feed_dict={self.inp : inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimension 1 in both shapes must be equal, but are 172 and 512 for 'concat_1' (op: 'ConcatV2') with input shapes: [?,172,172,64], [?,512,512,32], [] and with computed input tensors: input[2] = <3>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-f68705223e4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIntrinsicNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_dimensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-3243d8953cf4>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dimensions, dimensions, learning_rate)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mencoder_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_encoder_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mmid_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mid_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_decoder_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmid_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquared_difference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpected_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-3243d8953cf4>\u001b[0m in \u001b[0;36mget_decoder_output\u001b[0;34m(self, mid_outputs, encoder_layers)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlayer_num\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mencoder_layer_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlayer_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_layer_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                 \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.pyc\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1032\u001b[0m   return gen_array_ops._concat_v2(values=values,\n\u001b[1;32m   1033\u001b[0m                                   \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m                                   name=name)\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.pyc\u001b[0m in \u001b[0;36m_concat_v2\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m    517\u001b[0m   \"\"\"\n\u001b[1;32m    518\u001b[0m   result = _op_def_lib.apply_op(\"ConcatV2\", values=values, axis=axis,\n\u001b[0;32m--> 519\u001b[0;31m                                 name=name)\n\u001b[0m\u001b[1;32m    520\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    766\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    767\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2336\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2337\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2338\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2339\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1717\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1719\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1720\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    674\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimension 1 in both shapes must be equal, but are 172 and 512 for 'concat_1' (op: 'ConcatV2') with input shapes: [?,172,172,64], [?,512,512,32], [] and with computed input tensors: input[2] = <3>."
     ]
    }
   ],
   "source": [
    "network = IntrinsicNetwork((1024, 1024, 3), layer_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
